# 实验目的
掌握流处理数据发送模拟和接收后的统计处理
# 实验原理
模拟大数据的流处理分析中实时数据的发送和接收，创建一个网络流量截获器，捕捉本地网卡的数据包并写入kafka消息队列。编写一个读取消息的程序，实现对流量数据的长度统计。
# 实验步骤
在实例演示中模拟实际情况，需要源源不断地接入流数据，为了在演示过程中更接近真实环境将定义一个网络流量截获器。主要功能：通过截获本地网卡的数据包，不断将数据包的内容以kafka消息的方式写入消息队列。

网络包模型java代码
```
package com.inforstack.pcap;

public class PacketModel {
	
	private long ts;
	
	private String srcAddr;
	
	private String dstAddr;
	
	private String ipType;
	
	private int srcPort;
	
	private int dstPort;
	
	private String type;
	
	private String application;
	
	private int length;

	public long getTs() {
		return ts;
	}

	public void setTs(long ts) {
		this.ts = ts;
	}

	public String getSrcAddr() {
		return srcAddr;
	}

	public void setSrcAddr(String srcAddr) {
		this.srcAddr = srcAddr;
	}

	public String getDstAddr() {
		return dstAddr;
	}

	public void setDstAddr(String dstAddr) {
		this.dstAddr = dstAddr;
	}

	public String getIpType() {
		return ipType;
	}

	public void setIpType(String ipType) {
		this.ipType = ipType;
	}
	
	public int getSrcPort() {
		return srcPort;
	}

	public void setSrcPort(int srcPort) {
		this.srcPort = srcPort;
	}

	public int getDstPort() {
		return dstPort;
	}

	public void setDstPort(int dstPort) {
		this.dstPort = dstPort;
	}

	public String getType() {
		return type;
	}

	public void setType(String type) {
		this.type = type;
	}

	public String getApplication() {
		return application;
	}

	public void setApplication(String application) {
		this.application = application;
	}

	public int getLength() {
		return length;
	}

	public void setLength(int length) {
		this.length = length;
	}

}
```

网络包转换java代码
```
package com.inforstack.pcap;

import java.util.Iterator;

import org.pcap4j.packet.EthernetPacket;
import org.pcap4j.packet.EthernetPacket.EthernetHeader;
import org.pcap4j.packet.IpV4Packet;
import org.pcap4j.packet.IpV4Packet.IpV4Header;
import org.pcap4j.packet.IpV6Packet;
import org.pcap4j.packet.IpV6Packet.IpV6Header;
import org.pcap4j.packet.Packet;
import org.pcap4j.packet.TcpPacket;
import org.pcap4j.packet.TcpPacket.TcpHeader;
import org.pcap4j.packet.UdpPacket;
import org.pcap4j.packet.UdpPacket.UdpHeader;
import org.slf4j.Logger;

public class PacketModelBuilder {

	private static PacketModelBuilder instance = new PacketModelBuilder();
	
	public static PacketModelBuilder getInstance() {
		return instance;
	}

	private PacketModelBuilder() {
		
	};

	public PacketModel build(long ts, Packet packet, Logger log) {
		PacketModel model = new PacketModel();
		model.setTs(ts);
		model.setLength(packet.getRawData().length);
		Iterator<Packet> it = packet.iterator();
		while (it.hasNext()) {
			Packet p = it.next();
			if (p instanceof EthernetPacket) {
				//log.info(p.getClass().getSimpleName());
				EthernetPacket ethernetPacket = (EthernetPacket) p;
				EthernetHeader header = ethernetPacket.getHeader();
				//log.info(header.getSrcAddr().toString());
				//log.info(header.getDstAddr().toString());
			} else if (p instanceof IpV4Packet) {
				//log.info(p.getClass().getSimpleName());
				IpV4Packet ipv4Packet = (IpV4Packet) p;
				IpV4Header header = ipv4Packet.getHeader();
				//log.info(header.getSrcAddr().getHostAddress());
				//log.info(header.getDstAddr().getHostAddress());
				//log.info(header.getProtocol().name());
				
				model.setSrcAddr(header.getSrcAddr().getHostAddress());
				model.setDstAddr(header.getDstAddr().getHostAddress());
				model.setType(header.getProtocol().name());
				model.setIpType("IPV4");
			} else if (p instanceof IpV6Packet) {
				//log.info(p.getClass().getSimpleName());
				IpV6Packet ipv6Packet = (IpV6Packet) p;
				IpV6Header header = ipv6Packet.getHeader();
				//log.info(header.getSrcAddr().getHostAddress());
				//log.info(header.getDstAddr().getHostAddress());
				//log.info(header.getProtocol().name());
				
				model.setSrcAddr(header.getSrcAddr().getHostAddress());
				model.setDstAddr(header.getDstAddr().getHostAddress());
				model.setType(header.getProtocol().name());
				model.setIpType("IPV6");
			} else if (p instanceof TcpPacket) {
				TcpPacket tcpPacket = (TcpPacket) p;
				TcpHeader header = tcpPacket.getHeader();
				//log.info(header.getSrcPort().toString());
				//log.info(header.getDstPort().toString());
				
				model.setSrcPort(header.getSrcPort().valueAsInt());
				model.setDstPort(header.getDstPort().valueAsInt());
			} else if (p instanceof UdpPacket) {
				UdpPacket udpPacket = (UdpPacket) p;
				UdpHeader header = udpPacket.getHeader();
				//log.info(header.getSrcPort().toString());
				//log.info(header.getDstPort().toString());
				
				model.setSrcPort(header.getSrcPort().valueAsInt());
				model.setDstPort(header.getDstPort().valueAsInt());
			} else {
				//log.info(p.getClass().getSimpleName());
				//log.info(p.getRawData().length + "");
				
				model.setApplication(p.getClass().getSimpleName());
			}
		}
		return model;
	}

}
```

截获器java代码
```
package com.inforstack.pcap;

import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.pcap4j.core.BpfProgram.BpfCompileMode;
import org.pcap4j.core.NotOpenException;
import org.pcap4j.core.PacketListener;
import org.pcap4j.core.PcapHandle;
import org.pcap4j.core.PcapNativeException;
import org.pcap4j.core.PcapNetworkInterface;
import org.pcap4j.core.PcapNetworkInterface.PromiscuousMode;
import org.pcap4j.core.Pcaps;
import org.pcap4j.packet.Packet;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class PacketProducer {
	
	private static final Logger log = LoggerFactory.getLogger(PacketProducer.class);
	
	public static void main( String[] args ) {
		ProducerArguments config = new ProducerArguments(args);
		
		if (config.isSuccess()) {
			String currentAddress = config.getLocalAddress();
			String broker = config.getBroker();
			String topic = config.getTopic();
			int count = config.getCount();
			Producer<Long, byte[]> producer = null;
			try {
	    		PcapNetworkInterface currentInterface = null;
				List<PcapNetworkInterface> devs = Pcaps.findAllDevs();
				for (PcapNetworkInterface pni : devs) {
					log.debug(pni.toString());
					if (pni.toString().contains(currentAddress)) {
						currentInterface = pni;
					}
				}
				
				if (currentInterface != null) {
					if (broker != null) {
						Properties props = new Properties();
						props.put("bootstrap.servers", broker);
						props.put("acks", "all");
						props.put("retries", 0);
						props.put("batch.size", 16384);
						props.put("linger.ms", 1);
						props.put("buffer.memory", 33554432);
						props.put("key.serializer", "org.apache.kafka.common.serialization.LongSerializer");
						props.put("value.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");

						producer = new KafkaProducer<Long, byte[]>(props);
					}					
			        
					log.info(currentInterface.toString());
					
					int snaplen = 256 * 1024;
					
					int timeout = 50;
					
					PcapHandle.Builder phb = new PcapHandle.Builder(currentInterface.getName()).snaplen(snaplen)
							.promiscuousMode(PromiscuousMode.PROMISCUOUS).timeoutMillis(timeout)
							.bufferSize(1 * 1024 * 1024);
					
					PcapHandle handle = phb.build();
					
					String filter = "ip and (not dst port 9092) and (not src port 9092)";
					
					handle.setFilter(filter, BpfCompileMode.OPTIMIZE);
					
					final Producer<Long, byte[]> final_producer = producer;
					final SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss.SSS");
					final String final_topic = topic;
					
					PacketListener listener = new PacketListener() {

						public void gotPacket(Packet packet) {
							if (final_producer != null) {								
								Date now = new Date();
								log.info(String.format("ts: %s     len: %5d", sdf.format(now), packet.getRawData().length));
								ProducerRecord<Long, byte[]> record = new ProducerRecord<Long, byte[]>(final_topic, now.getTime(), packet.getRawData());
								try {
									final_producer.send(record);
								} catch (Exception e) {
									e.printStackTrace();
								}
							}
						}
					};
					
					handle.loop(count, listener);
					
					handle.close();
				}
			} catch (PcapNativeException e) {
				e.printStackTrace();
			} catch (NotOpenException e) {
				e.printStackTrace();
			} catch (InterruptedException e) {
				e.printStackTrace();
			} finally {
				if (producer != null) {
					producer.close();
				}
			}
		}
	}
}
```
流量统计Java代码
```
package com.inforstack.pcap;

import java.util.Arrays;
import java.util.Properties;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.Producer;
import org.pcap4j.packet.EthernetPacket;
import org.pcap4j.packet.Packet;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class PacketConsumer extends Thread {
	
	private static final Logger log = LoggerFactory.getLogger(PacketConsumer.class);
	
	private static boolean running = false;
	
	private static AtomicInteger threadCounts = new AtomicInteger(0); 

	public static void main(String[] args) {
		ConsumerArguments config = new ConsumerArguments(args);
		
		if (config.isSuccess()) {
			String broker = config.getBroker();
			String topic = config.getTopic();
			String group = config.getGroup();
			
			Runtime.getRuntime().addShutdownHook(new Thread() {

				@Override
				public void run() {
					running = false;
					while (threadCounts.get() > 0) {
						try {
							Thread.sleep(500);
						} catch (InterruptedException e) {
							e.printStackTrace();
						}
					}
				}
				
			});
			
			int count = config.getCount();
			
			if (count <= 0) {
				count = 1;
			}
			
			log.info(String.format("Create %d thread(s) to fetch packet record", count));
			
			running = true;
			
			for (int i = 0; i < count ; i++) {
				PacketConsumer instance = new PacketConsumer(i, broker, topic, group);
	    		instance.start();
	    		threadCounts.incrementAndGet();
			}
		}
	}
	
	private String broker;
	
	private String topic;
	
	private String group;
	
	private KafkaConsumer<Long, byte[]> consumer;
	
	public PacketConsumer(int id, String broker, String topic, String group) {
		super(String.format("PacketConsumer-%d", id));
		this.broker = broker;
		this.topic = topic;
		this.group = group;
	}

	@Override
	public void run() {
		int size = 0;
		this.init();
		Producer<Long, byte[]> producer = null;
		while (running) {
			try {
				ConsumerRecords<Long, byte[]> records = consumer.poll(100);
				
				if (records.isEmpty()) {
					continue;
				} else {
					for (ConsumerRecord<Long, byte[]> record : records) {
						long ts = record.key();
						byte[] rawData = record.value();
						Packet packet = EthernetPacket.newPacket(rawData, 0, rawData.length);
						PacketModel model = PacketModelBuilder.getInstance().build(ts, packet, log);
						if (model != null) {
							size += model.getLength();
							System.err.println("Size: " + size);
						}					
					}
				}					
				Thread.sleep(1);
			} catch (Exception e) {
				e.printStackTrace();
			} finally {
				if (producer != null) {
					producer.close();
				}
			}
			
		}	
		consumer.close();
		log.info("Closed kafka client");
		threadCounts.decrementAndGet();
	}
	
	private void init() {
		Properties props = new Properties(); 
		props.put("bootstrap.servers", this.broker);
		props.put("group.id", this.group);
		props.put("enable.auto.commit", "true");
		props.put("auto.commit.interval.ms", "1000");
		props.put("session.timeout.ms", "30000");
		props.put("key.deserializer", "org.apache.kafka.common.serialization.LongDeserializer");
		props.put("value.deserializer", "org.apache.kafka.common.serialization.ByteArrayDeserializer");
		
		this.consumer = new KafkaConsumer<Long, byte[]>(props);
		this.consumer.subscribe(Arrays.asList(this.topic));
		
		log.info("Create kafka client");
	}

}
```
执行方法：
启动流数据消费者consumer:
1. 对流数据消费者项目右键->点击Run as->点击Run Configuration...
2. 在对话框中新建Java application配置。将该配置命名为PacketConsumer;
3. 选择main class为com.inforstack.pcap.PacketConsumer;
4. 在Argument标签里添加Program arguments:
-b localhost:9092 -c 10 -t inforstack -g group1
5. 点击Run。
![](/images/1-12_20180405110956.056.png)

![](/images/1-13_20180405111135.035.png)

启动流数据生产者producer：
1. 对流数据生产者项目右键->点击Run as->点击Run Configuration...
2. 在对话框中新建Java application配置。将该配置命名为PacketProducer;
3. 选择main class为com.inforstack.pcap.PacketProducer;
4. 在Argument标签里添加Program arguments:
-b localhost:9092 -c 5 -i 127.0.0.1 -t inforstack
5. 点击Run。
![](/images/1-14_20180405111347.047.png)

![](/images/1-15_20180405111350.050.png)
